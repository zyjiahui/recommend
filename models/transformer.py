"""
6个encoder，6个decoder， 不共享参数
encoder：输入，注意力机制，前馈网络
    1、 输入：embedding（word2vec） + 位置嵌入
    rnn梯度消失有什么不同？？？（面试题）  梯度和，总梯度 
    transformer是并行化的，所有输入一起处理，不像rnn自带时序关系，一个一个喂入模型，所以transformer增快了速度，忽略了单词的序列关系，所以引入位置编码
    位置编码：原文对于每个512维度的字，偶数位置采用sin，奇数位置采用cos，这样得到的512维度的位置编码与词向量的512维度embedding相加，得到512维度作为transformer的输入
    为什么位置编码是有作用的？？？ 

    2、注意力机制：基本的注意力机制 + 在trm中怎么操作
    注意力机制的公式？？？   一个向量 * 一个矩阵 -> 加权的和
    Q K V三个矩阵 
    怎么获取Q K V ？？？  在只有单词向量的情况下：单词向量X * Wq参数(Wk / Wv)  分别得到Q K V
    相似性怎么求：点乘、MLP多层网络、cos余弦相似
    为什么除以√dk？？？  qk相乘之后值很大，输入softmax之后可能趋于0或1，导致梯度消失，需要归一化
    矩阵操作而不是单个输入  多头注意力：多套W参数，得到不同的qkv向量组合，多个头的输出

    3、残差和normalization
    残差的作用 F(X) + X   链式求导法则 缓解梯度消失（公式推导）
    为什么使用layer normalization而不是batch normalization？？？   归一化的知识
    在nlp任务中，大多数都用LN，传统的BN在nlp任务中较差，为什么？？？
decoder：
    1、masked的多头注意力机制  当前词之后的单词都masked掉
    如果不进行masked，训练时全部的单词会为当前要预测输出的单词提供信息，但是在预测时，是对下一个词进行预测，这时没有它后面（未来）的单词提供信息，只有前面已经预测出的单词  （不加masked，模型在训练和预测时存在gap，预测结果不好）
    
    2、交互层 传统的多头注意力机制（encoder和decoder的交互）
    encoder生成的是k v矩阵，decoder生成的是q矩阵
    encoder的输出要输入到每一个decoder中，而不是只输入到第一个decoder中

"""